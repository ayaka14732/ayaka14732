# Meet Ayaka: A Passionate Researcher & Open Source Contributor

Hi there! I am Ayaka, a 24-year-old computer science, historical linguistics, and mathematics researcher.

I have a vibrant presence on GitHub, where I've established a multitude of open-source projects and have hosted several services and websites at my own expense, significantly contributing to the open-source community. My open-source contributions span various fields, including deep learning, natural language processing, language conservation, historical linguistics, and computational linguistics.

## Proficiency in Deep Learning

My expertise in deep learning is reflected in my familiarity with tools like [JAX](https://github.com/google/jax) and Google Cloud TPU. In addition to raising issues for them, I've utilised my profound understanding of them to create [TPU Starter](https://github.com/ayaka14732/tpu-starter), a comprehensive guide for Google Cloud TPU, which has been translated into Korean. To enhance the user experience with JAX, I developed [jax-smi](https://github.com/ayaka14732/jax-smi), a tool that enables real-time memory usage monitoring, providing a similar experience to that of nvidia-smi. My significant contributions led to the honour of receiving the [2023 Google Open Source Peer Bonus Award](https://opensource.googleblog.com/2023/05/google-open-source-peer-bonus-program-announces-first-group-of-winners-2023.html).

## Natural Language Processing Expertise

In the realm of Natural Language Processing, I have contributed to the [Hugging Face Transformers](https://github.com/huggingface/transformers) library and developed, trained, and released many NLP models. I've implemented and released [bart-base-jax](https://github.com/ayaka14732/bart-base-jax) and [llama-jax](https://github.com/ayaka14732/llama-jax) from scratch, providing high-quality open-source codebases to deep learning researchers and engineers as examples of implementing and training Transformer models using JAX.

Additionally, using NumPy, I implemented the BERT model from scratch and performed in-browser inference using Pyodide, thereby creating [TrAVis](https://github.com/ayaka14732/TrAVis), a BERT Attention Visualiser that runs entirely within a browser, providing an intuitive understanding of BERT's attention mechanism for researchers. I'm an early adopter of the latest large-scale language model—ChatGPT and have been studying it since its release. I co-authored [Better ChatGPT](https://github.com/ztjhz/BetterChatGPT), an open-source enhanced ChatGPT website that offers many advanced features through the ChatGPT API. The website has garnered over 5,000 stars and is being used by millions of users worldwide.

## Language Conservation Efforts

My expertise in NLP also extends to language conservation. I've trained the [BART model for Cantonese](https://github.com/ayaka14732/bart-base-cantonese), a low-resource language, and released it on the [Hugging Face Hub](https://huggingface.co/Ayaka/bart-base-cantonese). Building upon this, I proposed [TransCan](https://github.com/ayaka14732/TransCan), an English-to-Cantonese machine translation model, outperforming the state-of-the-art commercial machine translation product by 11.8 BLEU. The model has been released on GitHub, bringing benefits to both Cantonese and the wider low-resource NLP community.

In addition to language models, I've created and released several datasets. For instance, in the [LIHKG Scraper](https://github.com/ayaka14732/lihkg-scraper) project, I circumvented many layers of Cloudflare's restrictions to scrape LIHKG, one the most popular Cantonese forums in Hong Kong, resulting in a corpus of 172,937,863 unique sentences. I've also created English-Cantonese parallel corpora, [Words.hk](https://github.com/ayaka14732/wordshk-parallel-corpus) and [ABC Cantonese](https://github.com/ayaka14732/abc-cantonese-parallel-corpus).

Moreover, I developed web-scraping programs to regularly fetch the lists of [Wenchang News](https://github.com/ayaka14732/VunsioNewsList) and [Xingning Hakka News](https://github.com/ayaka14732/SNHakkaNews), making contributions to the conservation of the Hainanese and Xingning Hakka languages.

## Pioneering Contributions in Historical Linguistics

I've also made considerable contributions to the field of historical linguistics. I founded the open-source organisation, [nk2028](https://github.com/nk2028), attracting a community of experts in historical linguistics. Within nk2028, we have conducted pioneering research in the field of Middle Chinese phonology. We innovatively formalised the phonological positions of the Tshet-uinh phonological system as sextuplets. This allowed us to precisely analyse the phonetic evolution of the Chinese language. Moreover, in the process of putting this system into practice, we made explorations in describing the laws of sound changes. Initially, we designed [a Domain-Specific Language](https://github.com/nk2028/purescript-qieyun) in PureScript and utilised SQLite as the [backend](https://github.com/nk2028/qieyun-sqlite) to handle queries. In subsequent research, we simplified our approach by creatively designing a [JavaScript library](https://github.com/nk2028/qieyun-js). This greatly enhanced productivity, reduced learning costs, and achieved remarkable success.

Based on this, we released the [Qieyun Autoderiver](https://github.com/nk2028/qieyun-autoderiver) website, allowing community members to [contribute their own derivation schemes](https://github.com/nk2028/qieyun-examples). This website has effectively invigorated the community and attracted many people into this field. To help beginners master the Tshet-uinh phonological system, we also published many tools, such as a tool to [automate the process of puonq-tshet](https://github.com/nk2028/pyanxchet), a tool to [generate Tshet-uinh Flashcards](https://github.com/nk2028/tshet-uinh-flashcard), and a tool to [look up Tshet-uinh phonological positions](https://github.com/nk2028/qieyun-tools).

The nk2028 organisation also made significant contributions to other aspects of linguistics. For instance, in the field of dialectology, we took over the discontinued MCPDict project and released the [Chinese Dialect Pronunciation Atlas](https://github.com/nk2028/hdqt). Regarding classical Chinese, with the consent of the data provider, Sou-Yun website, we published [ORCHESTRA](https://github.com/nk2028/ORCHESTRA-dataset), a comprehensive dataset of classical Chinese poetry. For phonetics, we created an [IPA Online Practice System](https://github.com/nk2028/ipa-practise) and a [Putonghua IPA Converter](https://github.com/nk2028/putonghua-ipa-converter) for annotating Mandarin IPA on Chinese text.<!-- TODO: We also released the uyghur project, encompassing resources about the Uyghur language. -->

## Innovations in Computational Linguistics

In the field of computational linguistics, I maintained the simplified-traditional Chinese conversion project [OpenCC](https://github.com/BYVoid/OpenCC) and its successor [StarCC](https://github.com/StarCC0), which can accurately handle the issues of one-to-many mappings in simplified-traditional Chinese conversion. Building on this, leveraging my in-depth understanding of OpenType font features, I proposed a novel approach for simplified-to-traditional conversion fonts to handle the one-to-many mappings, and released two fonts, [Fan Wun Ming](https://github.com/ayaka14732/FanWunMing) and [Fan Wun Hak](https://github.com/ayaka14732/FanWunHak), based on this method. The approach I used was also adopted by other font developers, thus enriching the typographic community.

For Cantonese, I published [cantoseg](https://github.com/ayaka14732/cantoseg), an effective Cantonese segmentation tool. I have also created two tools, namely [ToJyutping](https://github.com/CanCLID/ToJyutping) and [Inject Jyutping](https://github.com/CanCLID/inject-jyutping), which aid Cantonese learners in mastering the pronunciation of Chinese characters.

I am an active contributor to the [rime](https://github.com/rime) open-source input method community. As a member of the [CanCLID](https://github.com/CanCLID) organisation, I maintain the rime input schema for Cantonese, [rime-cantonese](https://github.com/rime/rime-cantonese). Furthermore, I've released input schemata for [TUPA](https://github.com/nk2028/rime-tupa), [Loengfan](https://github.com/CanCLID/rime-loengfan), [Mandarin](https://github.com/ayaka14732/rime-putonghua), and [Nüshu](https://github.com/nushu-script/rime-nushu). I've also curated [awesome-rime](https://github.com/ayaka14732/awesome-rime), a comprehensive list of rime IME schemata.

## Miscellaneous Endeavours and Contributions

My open-source contributions extend to my other areas of interest as well. With a deep understanding of the x64 instruction set, I crafted the [smallest 64-Bit PE file on Windows 10](https://github.com/ayaka14732/TinyPE-on-Win10) using assembly language. It's a Windows executable of merely 268 bytes that can run and pop up a message box. Moreover, I proposed the [Nya Calendar](https://github.com/ayaka14732/nya-calendar), a lunisolar-mercurial calendar that considers the synodic period of the Earth and Mercury, encompassing several unique properties.

In addition, I have contributed to the Arch Linux community by [maintaining several AUR packages](https://github.com/ayaka14732/AUR). I host several open-source websites and services at my own expense, including the [Online Nushu Dictionary](https://github.com/nushu-script/nushu-script.github.io) website, a [Graphviz server](https://github.com/ayaka14732/graphviz-server), a [Telegram translation bot](https://github.com/ayaka14732/telegram-translate-bot), and an instance of the [Shieldy bot](https://github.com/ayaka14732/shieldy).

If you want to know more about me and explore my other passions and interests, feel free to visit my [personal website](https://en.ayaka.shn.hk/)!
